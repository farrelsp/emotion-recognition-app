{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ae3602-42eb-419d-8763-4b75d7d504ea",
   "metadata": {},
   "source": [
    "# Emotion Recognition\n",
    "The dataset has 8 labels: sadness, anger, surprise, fear, joy, disgust, trust, anticipation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01a6eb3-2ff0-49da-8313-25242137c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utils.forward_fn import forward_sequence_classification\n",
    "from utils.metrics import document_sentiment_metrics_fn\n",
    "from utils.data_utils import EmotionDetectionDataset, EmotionDetectionDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf3a852-abd7-479c-8115-9288e00234b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is cuda available? True\n",
      "Device count? 4\n",
      "Current device? 0\n",
      "Device name?  NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Is cuda available?\", torch.cuda.is_available())\n",
    "print(\"Device count?\", torch.cuda.device_count())\n",
    "print(\"Current device?\", torch.cuda.current_device())\n",
    "print(\"Device name? \", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19196564-dea2-40ce-9953-d956972b1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add6718f-f66e-4758-bf4a-82a78a62f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common functions\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3436b4e7-a0dc-40b7-8f72-235aa7205f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "set_seed(26092020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75008b3d-2f97-4096-944c-51b1ebc97ee6",
   "metadata": {},
   "source": [
    "## Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4b52d7-fcb5-45f5-8ba2-fa304a6782a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-large-p1')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-large-p1')\n",
    "config.num_labels = EmotionDetectionDataset.NUM_LABELS\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-large-p1', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be460382-6a66-4502-8cbd-a2ca31a65c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7f60bd5-cb86-464e-9acc-6da96bcb7894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335150088"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_param(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f836e-b32c-4141-a654-ce4345bd3c16",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4654710a-0913-4c38-9382-55797c36c25c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7200 entries, 0 to 7199\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   7199 non-null   object\n",
      " 1   label   7200 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 112.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./dataset/emotion.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed864c98-8c13-4afd-90f8-3888a1564f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7199 entries, 0 to 7199\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   7199 non-null   object\n",
      " 1   label   7199 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 168.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(axis=0, how=\"any\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d1316f2-d33b-4c15-9c14-77f31f65f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb313798-ef6b-4889-b370-5c9caf7e8e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5759 entries, 2034 to 860\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   5759 non-null   object\n",
      " 1   label   5759 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 135.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75ca22b5-c0da-4e84-a445-d1f4e4f3b470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1440 entries, 4899 to 3340\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   1440 non-null   object\n",
      " 1   label   1440 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 33.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a878e91d-01e1-4290-b32a-36cff4feb5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['disgust', 'fear', 'surprise', 'anticipation', 'anger', 'joy',\n",
       "       'trust', 'sadness'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3020c52-ecd5-497b-a50a-f843c00feb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('./dataset/train.csv', index=False)\n",
    "df_test.to_csv('./dataset/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80a533fd-6161-493a-b2e4-b9adadc240a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './dataset/train.csv'\n",
    "test_dataset_path = './dataset/test.csv'\n",
    "\n",
    "train_dataset = EmotionDetectionDataset(train_dataset_path, tokenizer, lowercase=True)\n",
    "test_dataset = EmotionDetectionDataset(test_dataset_path, tokenizer, lowercase=True)\n",
    "\n",
    "train_loader = EmotionDetectionDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)   \n",
    "test_loader = EmotionDetectionDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79df7a48-22bc-4641-a1d9-da319f7b1dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0, 'anger': 1, 'surprise': 2, 'fear': 3, 'joy': 4, 'disgust': 5, 'trust': 6, 'anticipation': 7}\n",
      "{0: 'sadness', 1: 'anger', 2: 'surprise', 3: 'fear', 4: 'joy', 5: 'disgust', 6: 'trust', 7: 'anticipation'}\n"
     ]
    }
   ],
   "source": [
    "w2i, i2w = EmotionDetectionDataset.LABEL2INDEX, EmotionDetectionDataset.INDEX2LABEL\n",
    "print(w2i)\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a48903d-06f4-448e-9413-e704baae1d91",
   "metadata": {},
   "source": [
    "## Test sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4b85c91-1feb-47ed-a702-1a5804847f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita | Label : trust (24.821%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26c76dee-e70e-48c8-b18a-b17ed9c49e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Budi pergi ke pondok indah mall membeli cakwe | Label : trust (23.743%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Budi pergi ke pondok indah mall membeli cakwe'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a04fc2f-5dba-42f8-aafa-ce9b2e18a976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Dasar anak sialan!! Kurang ajar!! | Label : joy (25.949%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Dasar anak sialan!! Kurang ajar!!'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc965ab-c605-42e1-afff-9590da40544b",
   "metadata": {},
   "source": [
    "## Finetuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1617486c-cd4d-4591-9de9-723f61a6758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=5e-6)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f4e20b9-8c7f-4b0f-864c-7dd3859edad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save checkpoint\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "533f3a16-5b36-4a74-848d-f5d49d00b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './models/ckpt_ep_{}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cea7763c-d120-40b6-b4fa-31b890a0c7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.8126 LR:0.00000500: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [01:25<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.8126 ACC:0.76 F1:0.76 REC:0.76 PRE:0.76 LR:0.00000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7432 ACC:0.75 F1:0.75 REC:0.75 PRE:0.75: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:09<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.7432 ACC:0.75 F1:0.75 REC:0.75 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5366 LR:0.00000500: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [01:30<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5366 ACC:0.84 F1:0.84 REC:0.84 PRE:0.84 LR:0.00000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6688 ACC:0.78 F1:0.78 REC:0.78 PRE:0.78: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:10<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.6688 ACC:0.78 F1:0.78 REC:0.78 PRE:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.3848 LR:0.00000500: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [01:31<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.3848 ACC:0.88 F1:0.88 REC:0.88 PRE:0.88 LR:0.00000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6425 ACC:0.79 F1:0.79 REC:0.79 PRE:0.79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:10<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.6425 ACC:0.79 F1:0.79 REC:0.79 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.2599 LR:0.00000500: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [01:32<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.2599 ACC:0.93 F1:0.93 REC:0.93 PRE:0.93 LR:0.00000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6521 ACC:0.79 F1:0.79 REC:0.79 PRE:0.79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:08<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.6521 ACC:0.79 F1:0.79 REC:0.79 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.1763 LR:0.00000500: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [01:25<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.1763 ACC:0.96 F1:0.96 REC:0.96 PRE:0.96 LR:0.00000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6717 ACC:0.80 F1:0.80 REC:0.79 PRE:0.80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:09<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6717 ACC:0.80 F1:0.80 REC:0.79 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.1117 LR:0.00000500: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [01:29<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.1117 ACC:0.98 F1:0.98 REC:0.98 PRE:0.98 LR:0.00000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7009 ACC:0.80 F1:0.80 REC:0.80 PRE:0.80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:09<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.7009 ACC:0.80 F1:0.80 REC:0.80 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.0776 LR:0.00000500: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [01:30<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.0776 ACC:0.98 F1:0.98 REC:0.98 PRE:0.98 LR:0.00000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7585 ACC:0.79 F1:0.79 REC:0.79 PRE:0.79: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:09<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.7585 ACC:0.79 F1:0.79 REC:0.79 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.0515 LR:0.00000500: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [01:29<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.0515 ACC:0.99 F1:0.99 REC:0.99 PRE:0.99 LR:0.00000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7678 ACC:0.80 F1:0.80 REC:0.80 PRE:0.80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:08<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.7678 ACC:0.80 F1:0.80 REC:0.80 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.0347 LR:0.00000500: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [01:30<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.0347 ACC:0.99 F1:0.99 REC:0.99 PRE:0.99 LR:0.00000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7975 ACC:0.80 F1:0.80 REC:0.80 PRE:0.80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:09<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.7975 ACC:0.80 F1:0.80 REC:0.80 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.0285 LR:0.00000500: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [01:31<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.0285 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00 LR:0.00000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.8325 ACC:0.80 F1:0.80 REC:0.80 PRE:0.80: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:10<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.8325 ACC:0.80 F1:0.80 REC:0.80 PRE:0.80\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    " \n",
    "    total_train_loss = 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + tr_loss\n",
    "\n",
    "        # Calculate metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), get_lr(optimizer)))\n",
    "\n",
    "    # Calculate train metric\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    "\n",
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]        \n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "        \n",
    "        # Calculate total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss = total_loss + valid_loss\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "\n",
    "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "        \n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_loss/(i+1), metrics_to_string(metrics)))\n",
    "\n",
    "save_checkpoint(model, optimizer, epoch+1, total_loss/(i+1), checkpoint_path.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b1c4d6e-c2cf-4a2e-b6c4-99c32e928138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/tokenizer_config.json',\n",
       " './models/special_tokens_map.json',\n",
       " './models/vocab.txt',\n",
       " './models/added_tokens.json')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./models\")\n",
    "tokenizer.save_pretrained(\"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3a370-3dbb-43d3-b1cb-67c66357de8c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c493cf-c179-4d6d-9a8e-46539f4d5564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:08<00:00,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics:\n",
      "ACC: 0.8006944444444445\n",
      "F1: 0.8009739976113599\n",
      "REC: 0.8018016901607073\n",
      "PRE: 0.8007761109483404\n",
      "Number of unique labels in list_label_idx: 8\n",
      "Length of target_names: 8\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.78      0.83      0.80       159\n",
      "       anger       0.81      0.83      0.82       168\n",
      "    surprise       0.75      0.75      0.75       196\n",
      "        fear       0.91      0.89      0.90       180\n",
      "         joy       0.85      0.80      0.82       206\n",
      "     disgust       0.78      0.81      0.79       175\n",
      "       trust       0.76      0.76      0.76       178\n",
      "anticipation       0.77      0.74      0.75       178\n",
      "\n",
      "    accuracy                           0.80      1440\n",
      "   macro avg       0.80      0.80      0.80      1440\n",
      "weighted avg       0.80      0.80      0.80      1440\n",
      "\n",
      "     True Label Predicted Label\n",
      "0           joy             joy\n",
      "1         anger           anger\n",
      "2      surprise        surprise\n",
      "3          fear            fear\n",
      "4       disgust         disgust\n",
      "...         ...             ...\n",
      "1435      anger           anger\n",
      "1436    disgust         disgust\n",
      "1437   surprise        surprise\n",
      "1438    sadness         sadness\n",
      "1439      trust             joy\n",
      "\n",
      "[1440 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_and_report(model, valid_loader, i2w):\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "        \n",
    "        # Collect predictions and labels\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "    # Convert labels to index\n",
    "    # labels = list(range(len(target_names)))  # Adjust this as needed\n",
    "\n",
    "    list_hyp_idx = [EmotionDetectionDataset.LABEL2INDEX[hyp] for hyp in list_hyp]\n",
    "    list_label_idx = [EmotionDetectionDataset.LABEL2INDEX[label] for label in list_label]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp_idx, list_label_idx)\n",
    "    print(\"\\nMetrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Generate classification report\n",
    "    target_names = [EmotionDetectionDataset.INDEX2LABEL[i] for i in range(EmotionDetectionDataset.NUM_LABELS)]\n",
    "    \n",
    "    print(\"Number of unique labels in list_label_idx:\", len(set(list_label_idx)))\n",
    "    print(\"Length of target_names:\", len(target_names))\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(list_label_idx, list_hyp_idx, target_names=target_names))\n",
    "\n",
    "    # Create a DataFrame with true labels and predicted labels\n",
    "    df_results = pd.DataFrame({\n",
    "        'True Label': list_label,\n",
    "        'Predicted Label': list_hyp\n",
    "    })\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "df_results = evaluate_and_report(model, test_loader, i2w)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40e3278f-a580-4c72-a109-c321d4f91d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Label</th>\n",
       "      <th>Predicted Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>surprise</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>surprise</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>trust</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>surprise</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>anticipation</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>anticipation</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>disgust</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>trust</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>joy</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>trust</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>287 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        True Label Predicted Label\n",
       "5         surprise           anger\n",
       "6         surprise             joy\n",
       "24           trust           anger\n",
       "56        surprise           trust\n",
       "59    anticipation           trust\n",
       "...            ...             ...\n",
       "1420  anticipation           anger\n",
       "1424       disgust         sadness\n",
       "1430         trust    anticipation\n",
       "1431           joy           trust\n",
       "1439         trust             joy\n",
       "\n",
       "[287 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[df_results['True Label']!=df_results['Predicted Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "495e9cfa-c406-4369-9c19-b9e524825816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False predictions: 287/1440\n",
      "% False preds: 0.19930555555555557\n"
     ]
    }
   ],
   "source": [
    "false_preds = len(df_results[df_results['True Label']!=df_results['Predicted Label']])\n",
    "\n",
    "print(f\"False predictions: {false_preds}/{len(df_results)}\")\n",
    "print(f\"% False preds: {false_preds/len(df_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c2f9d-32a5-48a6-ac5b-157540bb1385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b13bb75-501f-440a-b162-b9b9ec4295c2",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a5ba2bf-a09f-4f8a-8b58-4a54a21aa8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter-23522009/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/jupyter-23522009/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Pastikan NLTK sudah mengunduh stopwords dan punktuasi\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Inisialisasi stop words untuk bahasa Indonesia\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Kamus emotikon ke emosi dalam bahasa Indonesia\n",
    "emoticon_dict = {\n",
    "    \"üòä\": \"ekspresi senang\",\n",
    "    \"ü§£\": \"ekspresi senang\",\n",
    "    \"üòÉ\": \"ekspresi senang\",\n",
    "    \"üòÑ\": \"ekspresi senang\",\n",
    "    \"üòÇ\": \"ekspresi senang\",\n",
    "    \"üòÅ\": \"ekspresi senang\",\n",
    "    \"üòÜ\": \"ekspresi senang\",\n",
    "    \"üòç\": \"ekspresi senang\",\n",
    "    \"ü§ó\": \"ekspresi senang\",\n",
    "    \"ü§®\": \"ekspresi kaget\",\n",
    "    \"üòØ\": \"ekspresi kaget\",\n",
    "    \"üòÆ\": \"ekspresi kaget\",\n",
    "    \"ü§¢\": \"ekspresi jijik\",\n",
    "    \"ü§Æ\": \"ekspresi jijik\",\n",
    "    \"üò∑\": \"ekspresi jijik\",\n",
    "    \"üòñ\": \"ekspresi jijik\",\n",
    "    \"üò´\": \"ekspresi jijik\",\n",
    "    \"üò©\": \"ekspresi jijik\",\n",
    "    \"üò≤\": \"ekspresi kaget\",\n",
    "    \"ü§Ø\": \"ekspresi kaget\",\n",
    "    \"üò¢\": \"ekspresi sedih\",\n",
    "    \"üò≠\": \"ekspresi sedih\",\n",
    "    \"üòû\": \"ekspresi sedih\",\n",
    "    \"üòî\": \"ekspresi sedih\",\n",
    "    \"üòü\": \"ekspresi sedih\",\n",
    "    \"üòï\": \"ekspresi sedih\",\n",
    "    \"üò¶\": \"ekspresi sedih\",\n",
    "    \"üòø\": \"ekspresi sedih\",\n",
    "    \"ü§ù\": \"ekspresi percaya\",\n",
    "    \"üëç\": \"ekspresi percaya\",\n",
    "    \"üôè\": \"ekspresi percaya\",\n",
    "    \"ü§≤\": \"ekspresi antisipasi\",\n",
    "    \"üò°\": \"ekspresi marah\",\n",
    "    \"üò†\": \"ekspresi marah\",\n",
    "    \"ü§¨\": \"ekspresi sangat marah\",\n",
    "    \"üò§\": \"ekspresi marah\",\n",
    "    \"üòæ\": \"ekspresi marah\",\n",
    "    \"üò®\": \"ekspresi takut\",\n",
    "    \"üò∞\": \"ekspresi takut\",\n",
    "    \"üò•\": \"ekspresi takut\",\n",
    "    \"üò±\": \"ekspresi takut\",\n",
    "    \":')\": \"ekspresi senang\",\n",
    "    \":)\": \"ekspresi senang\",\n",
    "    \":D\": \"ekspresi senang\",\n",
    "    \":(\": \"ekspresi sedih\",\n",
    "    \":'(\": \"ekspresi sedih\",\n",
    "    \":-)\": \"ekspresi senang\",\n",
    "    \":-D\": \"ekspresi senang\",\n",
    "    \":-(\": \"ekspresi sedih\",\n",
    "    \":P\": \"ekspresi bahagia\",\n",
    "    \";)\": \"ekspresi senang\",\n",
    "    \":-O\": \"ekspresi kaget\",\n",
    "    \":O\": \"ekspresi kaget\",\n",
    "}\n",
    "\n",
    "def replace_emoticons(text, emoticon_dict):\n",
    "    for emoticon, emotion in emoticon_dict.items():\n",
    "        if emoticon in text:\n",
    "            # Tambah koma sebelum deskripsi emosi jika ada teks sebelum emotikon\n",
    "            text = re.sub(r'(\\S)(' + re.escape(emoticon) + r')', r'\\1, ' + emotion, text)\n",
    "            # Ganti emotikon yang berdiri sendiri tanpa tambahan koma\n",
    "            text = text.replace(emoticon, emotion)\n",
    "    return text\n",
    "\n",
    "def clean_tweet(tweet, emoticon_dict):\n",
    "    # Mengubah teks menjadi huruf kecil\n",
    "    tweet = tweet.lower()\n",
    "    # Menghapus URL\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    # Menghapus mention (@username) dan hashtag\n",
    "    tweet = re.sub(r'@\\w+|#', '', tweet)\n",
    "    # Menghapus angka\n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "    # Gantikan emotikon dengan deskripsi emosi terlebih dahulu\n",
    "    tweet = replace_emoticons(tweet, emoticon_dict)\n",
    "    # Menghapus tanda baca dan simbol kecuali yang ada dalam deskripsi emotikon\n",
    "    allowed_punctuation = ''.join(re.escape(c) for c in emoticon_dict.values())\n",
    "    tweet = re.sub(r'[^\\w\\s' + allowed_punctuation + r']', '', tweet)\n",
    "    # Menghapus karakter non-ASCII\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', '', tweet)\n",
    "    # Menghapus spasi berlebih\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cde5d29-f190-43e4-8c72-2ec9c46ccbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ü§¢ | Label : disgust (99.725%)\n"
     ]
    }
   ],
   "source": [
    "text = 'ü§¢'\n",
    "cleaned_text = clean_tweet(text, emoticon_dict)\n",
    "subwords = tokenizer.encode(cleaned_text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87ee1946-ae1f-4f8f-8602-56f7f75c07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: üò° | Label : anger (99.894%)\n"
     ]
    }
   ],
   "source": [
    "text = 'üò°'\n",
    "cleaned_text = clean_tweet(text, emoticon_dict)\n",
    "subwords = tokenizer.encode(cleaned_text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a0349cd-6d5f-426a-8449-2cc6e1d31ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: üò± | Label : fear (68.661%)\n"
     ]
    }
   ],
   "source": [
    "text = 'üò±'\n",
    "cleaned_text = clean_tweet(text, emoticon_dict)\n",
    "subwords = tokenizer.encode(cleaned_text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "713539d4-e8ae-437a-9bed-61e336daa2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: woy aku beneran dapat hadiah ?? | Label : surprise (99.845%)\n"
     ]
    }
   ],
   "source": [
    "text = 'woy aku beneran dapat hadiah ??'\n",
    "cleaned_text = clean_tweet(text, emoticon_dict)\n",
    "subwords = tokenizer.encode(cleaned_text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5828570-7e55-415c-bf17-ec68d32eab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: nasib jomblo :( | Label : sadness (99.857%)\n"
     ]
    }
   ],
   "source": [
    "text = 'nasib jomblo :('\n",
    "cleaned_text = clean_tweet(text, emoticon_dict)\n",
    "subwords = tokenizer.encode(cleaned_text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bf5b4e2-58f0-4eed-a53f-66d7064c6e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: kalau aku salah pencet gimana gaiss, takut bgt uang nya hangus | Label : fear (99.916%)\n"
     ]
    }
   ],
   "source": [
    "text = 'kalau aku salah pencet gimana gaiss, takut bgt uang nya hangus'\n",
    "cleaned_text = clean_tweet(text, emoticon_dict)\n",
    "subwords = tokenizer.encode(cleaned_text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66c993d5-0b49-42a4-a91d-7c649a1aca44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: coba deh kamu taya dia, soalnya dia biasanya jujur | Label : trust (99.420%)\n"
     ]
    }
   ],
   "source": [
    "text = \"coba deh kamu taya dia, soalnya dia biasanya jujur\"\n",
    "cleaned_text = clean_tweet(text, emoticon_dict)\n",
    "subwords = tokenizer.encode(cleaned_text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00522dbd-bdcb-4cbc-8e17-c951b7642c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: bedak nya keputihan ga sih ? | Label : disgust (96.885%)\n"
     ]
    }
   ],
   "source": [
    "text = 'bedak nya keputihan ga sih ?'\n",
    "cleaned_text = clean_tweet(text, emoticon_dict)\n",
    "subwords = tokenizer.encode(cleaned_text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3082d3d2-6817-4f5d-9859-792be4ab455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ‚Å†makasih kakak | Label : joy (99.699%)\n"
     ]
    }
   ],
   "source": [
    "text = '‚Å†makasih kakak'\n",
    "cleaned_text = clean_tweet(text, emoticon_dict)\n",
    "subwords = tokenizer.encode(cleaned_text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73cc1166-7758-4e81-a594-2d113b14443d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: seharusnya ngomong nya di filter kak, jangan langsung gitu dong | Label : anger (58.365%)\n"
     ]
    }
   ],
   "source": [
    "text = 'seharusnya ngomong nya di filter kak, jangan langsung gitu dong'\n",
    "cleaned_text = clean_tweet(text, emoticon_dict)\n",
    "subwords = tokenizer.encode(cleaned_text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3db5446-8da6-466f-a6a4-a5221ab2df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ga sabar bgt besok nonton konser | Label : anticipation (99.864%)\n"
     ]
    }
   ],
   "source": [
    "text = 'ga sabar bgt besok nonton konser'\n",
    "cleaned_text = clean_tweet(text, emoticon_dict)\n",
    "subwords = tokenizer.encode(cleaned_text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ff92e-13e3-4baf-939e-d480c4173685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5f2b141-b821-4150-8b7c-6b90036f9031",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5da82755-f588-4aa9-ad11-29e62584585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models\"\n",
    "tokenizer_inf = BertTokenizer.from_pretrained(model_path)\n",
    "model_inf = BertForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b055dd26-1355-4b41-b25c-777648b6b5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ga sabar bgt besok nonton konser | Label : anticipation (99.625%)\n"
     ]
    }
   ],
   "source": [
    "new_text = \"pokoknya aku harus baca dulu acara nya, biar nanti ga salah\"\n",
    "cleaned_text = clean_tweet(new_text, emoticon_dict)\n",
    "subwords = tokenizer_inf.encode(cleaned_text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model_inf.device)\n",
    "\n",
    "logits = model_inf(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564458c-8b25-46b0-bda8-b55a4f80e736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
